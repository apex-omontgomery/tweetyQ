{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision with other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this experiment is to compare the Deep Learning model with other Conventional ML approaches to the same problem of clickbait detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string \n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: \n",
      "1 0 celebs killin the elf on the shelf game\n",
      "all the looks at the people ' s choice awards\n",
      "does kylie jenner know how to wear <UNK> ? a very serious investigation\n",
      "2 4 gifs that show how corinne is the greatest <UNK> bachelor <UNK> <UNK> yet\n",
      "all the looks at the 2 0 1 7 golden globes\n",
      "--------------------------------------------------\n",
      "Genuine: \n",
      "a <UNK> <UNK>\n",
      "bill to replace jallikattu ordinance will be placed in assembly : tn governor\n",
      "myanmar asks for time and space to solve rohingya crisis\n",
      "the nowhere people next door\n",
      "a tussle between judges and jallikattu supporters on who <UNK> more for bulls\n"
     ]
    }
   ],
   "source": [
    "genuine = open(\"../data/genuine.preprocessed.txt\").read().split(\"\\n\")\n",
    "clickbait = open(\"../data/clickbait.preprocessed.txt\").read().split(\"\\n\")\n",
    "\n",
    "\n",
    "print \"Clickbait: \"\n",
    "for each in clickbait[:5]:\n",
    "    print each\n",
    "print \"-\" * 50\n",
    "\n",
    "print \"Genuine: \"\n",
    "for each in genuine[:5]:\n",
    "    print each\n",
    "    \n",
    "\n",
    "data = clickbait + genuine\n",
    "labels = len(genuine) * [0] + len(clickbait) * [1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait: \n",
      "All The Looks At The People's Choice Awards\n",
      "Does Kylie Jenner Know How To Wear Coats? A Very Serious Investigation\n",
      "This Is What US Protests Looked Like In The '60s\n",
      "24 GIFs That Show How Corinne Is The Greatest \"Bachelor\" Villian Yet\n",
      "Nene Leakes And Kandi Burruss Finally \"See Each Other\" In A Good Way\n",
      "--------------------------------------------------\n",
      "Genuine: \n",
      "Mayawatis risky calculus\n",
      "L&T Q3 net up 39% at Rs 972 cr, co says note ban a disruptor\n",
      "Australian Open women's final: Serena beats sister Venus Williams to win 23rd Grand Slam\n",
      "It's Federer vs Nadal in Australian Open finals\n",
      "Medical board fails to make any conclusion in report on Sunandas death\n"
     ]
    }
   ],
   "source": [
    "clickbait_valid = open(\"../data/clickbait.valid.txt\").read().split(\"\\n\")\n",
    "genuine_valid = open(\"../data/genuine.valid.txt\").read().split(\"\\n\")\n",
    "\n",
    "print \"Clickbait: \"\n",
    "for each in clickbait_valid[:5]:\n",
    "    print each\n",
    "print \"-\" * 50\n",
    "\n",
    "print \"Genuine: \"\n",
    "for each in genuine_valid[:5]:\n",
    "    print each\n",
    "\n",
    "valid_data = clickbait_valid + genuine_valid\n",
    "vocabulary = open(\"../data/vocabulary.txt\").read().split(\"\\n\")\n",
    "inverse_vocabulary = dict((word, i) for i, word in enumerate(vocabulary))\n",
    "\n",
    "\n",
    "valid_data = [\" \".join([w if w in vocabulary else \"<UNK>\" for w in sent.split()]) for sent in valid_data]\n",
    "\n",
    "valid_labels = len(clickbait_valid) * [1] + len(genuine_valid) * [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([(\"vect\", CountVectorizer()),\n",
    "                     (\"tfidf\", TfidfTransformer()),\n",
    "                     (\"clf\",  SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_clf.fit(data, labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "MATCH_MULTIPLE_SPACES = re.compile(\"\\ {2,}\")\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "def words_to_indices(words):\n",
    "    return [inverse_vocabulary.get(word, inverse_vocabulary[UNK]) for word in words]\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, \" \" + punctuation + \" \")\n",
    "    for i in range(10):\n",
    "        text = text.replace(str(i), \" \" + str(i) + \" \")\n",
    "    text = MATCH_MULTIPLE_SPACES.sub(\" \", text)\n",
    "    return text\n",
    "\n",
    "model = load_model(\"../models/detector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = sequence.pad_sequences([words_to_indices(clean(sent.lower()).split()) for sent in valid_data], maxlen=SEQUENCE_LENGTH)\n",
    "predictions = model.predict(inputs)\n",
    "predictions = predictions.flatten() > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      1.00      0.66        74\n",
      "          1       0.00      0.00      0.00        76\n",
      "\n",
      "avg / total       0.24      0.49      0.33       150\n",
      "\n",
      "--------------------------------------------------\n",
      "Convolutional Neural Network\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.81      0.59        74\n",
      "          1       0.26      0.07      0.11        76\n",
      "\n",
      "avg / total       0.36      0.43      0.34       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"SVM\")\n",
    "print (metrics.classification_report(valid_labels, svm_clf.predict(valid_data)))\n",
    "\n",
    "print \"-\" * 50\n",
    "\n",
    "print (\"Convolutional Neural Network\")\n",
    "print (metrics.classification_report(valid_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
